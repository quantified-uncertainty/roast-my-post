# OpenAI AI Safety Research Agenda

**Sources:**
- [OpenAI Safety & Responsibility](https://openai.com/safety/)
- [Preparedness Framework Updates](https://openai.com/index/updating-our-preparedness-framework/)
- [Frontier Risk Approach](https://openai.com/global-affairs/our-approach-to-frontier-risk/)

**Last Updated:** 2024-2025

## Overview

OpenAI's safety approach centers on their Preparedness Framework for measuring and protecting against severe harm from frontier AI capabilities. Notable organizational changes in 2024 included the dissolution of the Superalignment team.

## Core Safety Principles

### 1. Embrace Uncertainty
Treat safety as a science with rigorous empirical investigation rather than certainty.

### 2. Defense in Depth
Stack multiple interventions for redundancy rather than relying on single safeguards.

### 3. Scalable Methods
Seek methods that scale with model capabilities rather than requiring constant human intervention.

### 4. Maintain Human Control
Preserve meaningful human oversight and decision-making authority.

### 5. Community Effort
View safety as a collaborative challenge requiring ecosystem-wide participation.

## Preparedness Framework

**Purpose:** Risk-based approach to responsible frontier model development, especially regarding catastrophic risks.

### Risk Categories Tracked

1. **Cybersecurity**
   - Offensive cyber capabilities
   - Vulnerability discovery and exploitation
   - Defense evasion

2. **Chemical & Biological Threats (CBRN)**
   - Dangerous knowledge acquisition
   - Synthesis planning
   - Dual-use biology

3. **Persuasion**
   - Influence operations
   - Misinformation generation
   - Manipulation capabilities

4. **Autonomy**
   - Self-replication
   - Resource acquisition
   - Independent operation

### Evaluation & Monitoring
- Rigorous frontier model capability evaluations
- Continuous monitoring during development
- Pre-deployment testing
- Post-deployment tracking

### Governance Structure
- Dedicated safety team oversight
- Accountability mechanisms
- Risk mitigation requirements
- Deployment decision processes

## Superalignment Program (2023-2024)

**Original Goal:** Solve superintelligence alignment within four years

**Approach:**
- Build near human-level automated alignment researcher
- Use significant compute allocation to scale alignment efforts
- Co-led by Ilya Sutskever and Jan Leike

**Status: DISSOLVED (2024)**

**Key Departures:**
- Ilya Sutskever (co-founder, chief scientist)
- Jan Leike (alignment co-lead)
- Multiple team members

**Jan Leike's Stated Concerns:**
- OpenAI not meeting publicly announced compute commitments
- Safety culture issues
- Insufficient prioritization of safety over capabilities

**Impact:** Major setback for OpenAI's alignment research credibility and long-term safety strategy.

## Current Safety Organization

**Teams:**
- Preparedness (risk assessment and mitigation)
- Safety systems (deployed model safety)
- Red teaming (adversarial testing)

**Note:** Structure post-Superalignment dissolution unclear; team rebuilt but with different leadership.

## Safety Scoring

**Independent Assessment (2025):** 33% safety score (second after Anthropic)

**Strengths:**
- Substantive dangerous capability testing (CBRN, cyber, persuasion)
- Preparedness Framework provides structure
- Some articulated AGI alignment strategy

**Gaps:**
- Superalignment dissolution undermines long-term credibility
- Limited transparency on safety processes
- Organizational turmoil suggests competing priorities

## Notable Safety Work

### GPT-4 Safety
- Extensive red-teaming before release
- RLHF for reducing harmful outputs
- Usage policies and monitoring

### Capability Evaluations
- ARA (autonomous replication and adaptation) testing
- Chemical/biological knowledge assessments
- Persuasion capability measurements

### Research Contributions
- RLHF methodology development
- Scaling laws for safety
- Adversarial robustness research

## Organizational Concerns

### Leadership Instability
Multiple high-profile safety-focused departures in 2024 raise questions about:
- Internal safety culture
- Resource allocation to safety vs capabilities
- Long-term commitment to safety research

### Competitive Pressures
OpenAI operates in highly competitive environment:
- Pressure to ship products quickly
- Revenue requirements as for-profit entity
- Balancing safety with commercial viability

### Governance Transition
Restructuring from nonprofit-controlled to more traditional corporate governance:
- Impact on safety prioritization unclear
- Board changes following 2023 crisis
- Investor influence on decision-making

## Key Uncertainties

1. **Post-Superalignment Strategy:** What replaces the dissolved alignment team?
2. **Compute Allocation:** Will safety research receive promised resources?
3. **Timeline Assumptions:** How much time does OpenAI think we have?
4. **AGI Deployment:** What are actual deployment criteria for AGI-level systems?

## Comparison to Other Labs

**More Transparent Than:** Meta, xAI
**Less Transparent Than:** Anthropic, DeepMind (on some metrics)

**Safety Investment:** Unclear post-2024 changes; historically significant but recent trends concerning

**Research Quality:** High-quality safety research output but quantity decreased after team departures

## External Assessments

**Concerns Raised by Former Staff:**
- Safety taking backseat to commercial concerns
- Insufficient resources for safety relative to capabilities
- Cultural shift away from safety prioritization

**Positive Indicators:**
- Still investing in dangerous capability evaluations
- Preparedness Framework continues to evolve
- Some safety research publications continue

## Open Questions for LLM Evaluation

1. **Organizational Credibility:** How should Superalignment dissolution affect confidence in OpenAI's safety commitment?
2. **Framework Effectiveness:** Is the Preparedness Framework sufficient for catastrophic risk prevention?
3. **Resource Allocation:** What fraction of OpenAI's resources should go to safety?
4. **Deployment Decisions:** How to evaluate whether OpenAI's deployment decisions prioritize safety appropriately?
