# Independent AI Safety Research Organizations

This document covers nonprofit and independent research organizations focused on AI safety, distinct from industry labs.

## MIRI (Machine Intelligence Research Institute)

**Founded:** 2000 (as Singularity Institute)
**Focus:** Mathematical foundations of AI alignment
**Status Update:** January 2024

### Mission & Strategy

**Historical Focus:**
- Agent foundations (formal decision theory)
- Embedded agency
- Logical uncertainty
- Corrigibility research

**2024 Strategy Shift:**
- CEO leadership transition (Nate Soares â†’ new leadership)
- Infrastructure and ecosystem support role
- Co-working space for AI x-risk organizations
- Helped establish Redwood Research and others

### Timeline Perspective

**MIRI Researcher Poll (Fall 2023):**
- Median AGI timeline: 9 years
- Mean AGI timeline: 14.6 years
- Reflects relatively short timelines within safety community

### Research Approach

**Characteristics:**
- Highly theoretical/mathematical
- Long-term research focus
- Pessimistic about alignment difficulty
- Advocates for caution and slowdown

**Key Contributions:**
- Agent foundations literature
- Alignment problem formalization
- Security mindset in AI safety
- Conceptual clarity on AGI risks

### Current Role

**Less Active in:**
- Direct technical research output
- Public-facing publications

**More Active in:**
- Ecosystem building
- Supporting other organizations
- Infrastructure provision
- Strategic guidance

---

## Redwood Research

**Founded:** ~2021
**Focus:** Pragmatic AI safety research with industry engagement

### Research Agenda Evolution

**Notable:** Agenda has pivoted several times, showing adaptability but also search for tractable approaches.

**Current Focus Areas:**
1. Threat assessment for AI systems
2. Mitigation techniques for identified threats
3. Evaluation methodologies
4. AI control techniques

### Key Research

**Recent Work:**
- "Towards evaluations-based safety cases for AI scheming" (collaboration with UK AISI, METR, Apollo, UC Berkeley)
- AI control protocols
- Adversarial robustness testing
- Red-teaming methodologies

### Organizational Model

**Characteristics:**
- Small, focused team
- Rapid iteration on research directions
- Close industry collaboration (esp. with Anthropic, DeepMind)
- Pragmatic orientation

**Industry Engagement:**
- Advised DeepMind on Frontier Safety Framework misalignment section
- Collaborated with multiple labs on evaluations
- Bridges academic and industry safety work

### Strengths

- Practical focus on near-term applicable techniques
- Strong industry connections
- Empirical research culture
- Collaborative approach

### Concerns Raised

- Multiple agenda pivots suggest difficulty finding traction
- Limited public research output relative to size
- Unclear long-term strategy

---

## Apollo Research

**Founded:** ~2022
**Focus:** AI scheming detection and evaluation

### Core Research Areas

**Scheming & Deception:**
- Detecting strategic deception in AI systems
- Evaluating alignment faking behaviors
- Testing for in-context scheming
- Threat modeling for deceptive AI

**Evaluation Methodologies:**
- Developing rigorous eval frameworks
- Safety case construction
- Red-teaming protocols
- Measurement techniques for subtle misalignment

### Key Collaborations

**OpenAI Partnership:**
- Tested training methods to reduce scheming behavior
- Empirical work on deception detection

**Multi-Organization Research:**
- UK AI Safety Institute
- METR
- Redwood Research
- UC Berkeley

**Industry Advisory:**
- Advised DeepMind on FSF misalignment section
- Consultation with multiple AI labs

### Research Approach

**Characteristics:**
- Empirical focus on concrete threat models
- Systematic evaluation development
- Evidence-based safety recommendations
- Collaborative research model

### Contributions

- Advanced understanding of AI scheming
- Practical evaluation tools
- Evidence of alignment faking in certain training setups
- Improved industry eval practices

---

## Conjecture

**Founded:** ~2021
**Focus:** Interpretability and conceptual alignment

### Original Research Agenda

**Core Areas:**
1. Interpretability research
2. Conceptual alignment
3. Epistemology of AI systems
4. Theoretical foundations

### Status & Concerns

**Note:** Search results primarily contained critiques rather than recent agenda information.

**Reported Issues:**
- Agenda clarity questions
- Execution challenges
- Team turnover
- Strategic direction uncertainty

**Positive Indicators:**
- Novel approaches to interpretability
- Conceptual work on alignment
- Attempts at ambitious research

### Current Status

**Unclear:** Limited public information on 2024-2025 activities and current focus.

---

## METR (Model Evaluation and Threat Research)

**Formerly:** ARC Evals
**Focus:** Dangerous capability evaluations

### Core Mission

Develop and conduct evaluations for:
- Autonomous replication and adaptation (ARA)
- Dangerous capabilities across domains
- AI-assisted catastrophic risks

### Evaluation Focus

**Key Assessments:**
- Can models self-replicate?
- Can models acquire resources autonomously?
- Can models conduct harmful activities end-to-end?
- What safeguards are effective against capable models?

### Industry Role

**Client Labs:**
- Anthropic
- OpenAI
- Google DeepMind
- Other frontier labs

**Services:**
- Pre-deployment evaluations
- Capability threshold testing
- Safety case validation
- Risk assessment

### Collaborative Research

Frequent co-authors with:
- Apollo Research
- Redwood Research
- UK AI Safety Institute
- Academic institutions

---

## Comparison Across Organizations

### Research Orientation Spectrum

**More Theoretical:**
- MIRI (agent foundations, math)
- Conjecture (conceptual)

**Balanced:**
- Apollo (scheming theory + empirical evals)
- METR (eval methodology + execution)

**More Empirical:**
- Redwood (pragmatic techniques)

### Industry Engagement

**High Engagement:**
- Redwood (deep industry collaboration)
- Apollo (multi-lab partnerships)
- METR (evaluation services)

**Medium Engagement:**
- MIRI (ecosystem support)
- Conjecture (unclear)

### Timeline Assumptions

**Shorter Timelines:**
- MIRI (9-15 years)
- Redwood (pragmatic urgency)
- Apollo (focus on near-term scheming)

**Longer/Unclear:**
- Conjecture
- METR (evaluation-focused regardless of timeline)

### Resource Scale

**Smaller:**
- Conjecture
- Apollo

**Medium:**
- Redwood
- METR

**Historical Significance:**
- MIRI (influential but now smaller research output)

---

## Ecosystem Role

### Complementarity

**MIRI:** Foundations and conceptual clarity
**Redwood:** Practical techniques and industry connection
**Apollo:** Specific threat model (scheming) deep-dive
**METR:** Evaluation infrastructure and services
**Conjecture:** Alternative interpretability approaches

### Collaboration Network

These organizations frequently:
- Co-author papers
- Share researchers
- Coordinate on projects
- Advise industry labs collectively

### Funding Ecosystem

Primary funders include:
- Open Philanthropy
- Long-Term Future Fund
- Survival and Flourishing Fund
- Individual donors (FTX impact now removed)

---

## For LLM Evaluation Experiments

### Testing Questions

1. **Approach Effectiveness:**
   - Is theoretical work (MIRI) or empirical work (Redwood) more valuable?
   - Should scheming (Apollo) be top priority vs broader evals (METR)?

2. **Organization Credibility:**
   - How should we weight agenda pivots (Redwood) vs consistency (MIRI)?
   - Does industry collaboration indicate effectiveness or capture?

3. **Resource Allocation:**
   - Which organizations deserve more funding?
   - Is ecosystem diversity valuable or wasteful?

4. **Strategic Direction:**
   - Should safety community focus on specific threats (Apollo) or broad capabilities?
   - Is coordination between orgs working well enough?

### Potential Biases to Test

**Founder Effect:**
- Models may favor organizations they've heard of (MIRI) over newer ones (Apollo)

**Approach Bias:**
- Technical models may favor empirical work over theory
- Or vice versa depending on training data

**Industry Alignment:**
- Do models view industry collaboration as positive (pragmatic) or negative (captured)?

**Self-Reference:**
- How do models evaluate orgs they may have trained on or been developed by people from?
