/**
 * Auto-generated tool README content
 * Generated by scripts/generate-tool-readmes.ts
 * DO NOT EDIT MANUALLY
 * 
 * README Hash: 28637f635bfa178f3718835df4b3469cc3c4d0061ce937e858f5f6c3e9aed5df
 */

export const toolReadmes = {
  "check-spelling-grammar": "# Spelling & Grammar Checker\n\nAn advanced AI-powered tool for detecting and correcting spelling and grammar errors in text. Provides detailed analysis with severity ratings, explanations, and improvement suggestions.\n\n## Overview\n\nThe Spelling & Grammar Checker uses sophisticated AI to:\n\n1. **Detect Errors** - Identifies spelling mistakes and grammatical issues\n2. **Classify Severity** - Rates errors as critical, major, or minor\n3. **Provide Corrections** - Suggests specific improvements for each error\n4. **Explain Issues** - Offers detailed explanations for learning\n5. **Track Metrics** - Reports total errors found and processing statistics\n\n## Key Features\n\n- **Dual Detection**: Catches both spelling and grammar errors\n- **Severity Classification**: Critical, major, and minor error categories\n- **Detailed Corrections**: Specific suggestions for each identified issue\n- **Educational Explanations**: Learn from mistakes with clear descriptions\n- **Line-by-Line Analysis**: Pinpoints exact location of errors\n- **Processing Metrics**: Track analysis performance and statistics\n\n## Error Severity Levels\n\n### Critical Errors (70+ importance)\n- Serious spelling mistakes that affect comprehension\n- Major grammatical errors that change meaning\n- Issues that significantly impact readability\n\n### Major Errors (40-70 importance)\n- Moderate spelling and grammar issues\n- Errors that affect flow but don't change core meaning\n- Common mistakes that should be corrected\n\n### Minor Errors (<40 importance)\n- Subtle grammatical inconsistencies\n- Style preferences rather than strict errors\n- Minor issues that don't significantly impact readability\n\n## Error Types Detected\n\n### Spelling Errors\n- Misspelled words and typos\n- Incorrect word forms\n- Missing or extra letters\n- Commonly confused words (their/there/they're)\n\n### Grammar Errors\n- Subject-verb disagreement\n- Incorrect verb tenses\n- Pronoun usage issues\n- Sentence structure problems\n- Punctuation errors\n\n## Best Practices\n\n1. **Review All Suggestions**: AI isn't perfect - evaluate each suggestion critically\n2. **Consider Context**: Some \"errors\" may be intentional stylistic choices\n3. **Check Specialized Terms**: Technical or domain-specific terms may be flagged incorrectly\n4. **Multiple Passes**: Run the tool multiple times after making corrections\n5. **Human Review**: Always have a final human review for important documents\n\n## Use Cases\n\n### Academic Writing\n```\nProofread essays, research papers, and academic publications for errors.\n```\n\n### Business Communication\n```\nEnsure emails, reports, and proposals are error-free and professional.\n```\n\n### Content Creation\n```\nPolish blog posts, articles, and marketing materials before publication.\n```\n\n### Learning and Education\n```\nUse detailed explanations to improve writing skills and language understanding.\n```\n\n## Limitations\n\n- May not catch all errors, especially context-dependent issues\n- Might flag intentional stylistic choices as errors\n- Less accurate with highly technical or specialized content\n- Cannot replace human proofreading for critical documents\n- Performance varies with text complexity and length",
  "extract-factual-claims": "# Extract Factual Claims Tool\n\nAn AI-powered tool for extracting and categorizing factual claims from text. Identifies verifiable statements and classifies them by type with confidence scores.\n\n## Overview\n\nThe Extract Factual Claims tool uses advanced AI to:\n\n1. **Identify Claims** - Extracts factual statements from text\n2. **Classify Types** - Categorizes claims as factual, statistical, historical, scientific, or other\n3. **Assess Verifiability** - Determines if claims can be verified through external sources\n4. **Rate Confidence** - Provides confidence scores for each extracted claim\n5. **Provide Context** - Includes contextual information where relevant\n\n## Key Features\n\n- **Multi-type Classification**: Factual, statistical, historical, scientific, and other claim types\n- **Verifiability Assessment**: Indicates whether claims can be independently verified\n- **Confidence Scoring**: Percentage confidence for extraction accuracy\n- **Contextual Information**: Additional context for better understanding\n- **Batch Processing**: Handles multiple claims in a single text input\n- **Performance Metrics**: Processing time and claim count statistics\n\n## Claim Types\n\n### Factual Claims\n- General factual statements about events, people, or things\n- Example: \"The capital of France is Paris\"\n\n### Statistical Claims\n- Numerical data, percentages, measurements, or statistics\n- Example: \"The unemployment rate increased by 2.3% in 2023\"\n\n### Historical Claims\n- Statements about past events, dates, or historical facts\n- Example: \"World War II ended in 1945\"\n\n### Scientific Claims\n- Scientific facts, research findings, or technical information\n- Example: \"Water boils at 100Â°C at sea level\"\n\n### Other Claims\n- Claims that don't fit into the above categories\n- May include subjective or opinion-based statements incorrectly identified as factual\n\n## Verifiability Assessment\n\n### Verifiable Claims\n- Can be checked against reliable external sources\n- Typically include specific facts, dates, numbers, or well-documented events\n- Suitable for fact-checking processes\n\n### Non-Verifiable Claims\n- Cannot be easily verified through standard sources\n- May include opinions, predictions, or subjective statements\n- Require careful evaluation before use in research\n\n## Best Practices\n\n1. **Review All Claims**: AI extraction may miss subtle nuances or context\n2. **Verify Important Claims**: Always fact-check extracted claims through reliable sources\n3. **Consider Context**: Understand the broader context in which claims are made\n4. **Check Confidence Scores**: Higher confidence indicates more reliable extraction\n5. **Validate Classification**: Ensure claim types are correctly categorized\n\n## Use Cases\n\n### Research Analysis\n```\nExtract factual claims from research papers, articles, or reports for analysis.\n```\n\n### Content Verification\n```\nIdentify claims in articles or documents that need fact-checking.\n```\n\n### Information Organization\n```\nSystematically organize factual information from large text documents.\n```\n\n### Quality Assurance\n```\nReview content to ensure all factual claims are properly supported.\n```\n\n## Integration Workflow\n\nThis tool works well in combination with:\n1. **Fact Checker Tool** - Verify extracted claims for accuracy\n2. **Perplexity Research Tool** - Find additional sources for claim verification\n3. **Link Validator Tool** - Check sources and references mentioned in claims\n\n## Limitations\n\n- May extract opinion statements as factual claims in some cases\n- Effectiveness varies with text complexity and domain specificity\n- Cannot verify the accuracy of extracted claims (only identifies them)\n- May miss implicit or contextually dependent claims\n- Performance may vary with different writing styles and formats\n\n## Example Output\n\nFor the input \"The Eiffel Tower is 324 meters tall and was completed in 1889\":\n\n- **Claim 1**: \"The Eiffel Tower is 324 meters tall\"\n  - Type: Statistical\n  - Verifiable: Yes\n  - Confidence: 95%\n\n- **Claim 2**: \"The Eiffel Tower was completed in 1889\"\n  - Type: Historical\n  - Verifiable: Yes\n  - Confidence: 98%",
  "fact-checker": "# Fact Checker Tool\n\nThe fact-checker tool is designed to verify the accuracy of specific factual claims. It works in tandem with the extract-factual-claims tool to provide a complete fact-checking pipeline.\n\n## How it Works\n\nThis tool takes a specific claim and verifies its accuracy by:\n\n1. Analyzing the claim for accuracy\n2. Providing a verdict (true, false, partially-true, unverifiable, outdated)\n3. Explaining the reasoning with evidence\n4. Suggesting corrections if needed\n\n## Separation of Concerns\n\nSimilar to the forecasting system, fact-checking is split into two tools:\n\n- **extract-factual-claims**: Finds and scores factual claims in documents\n- **fact-checker**: Verifies the accuracy of specific claims\n\nThis separation allows:\n- Better performance through parallel processing\n- Cleaner interfaces and testing\n- Flexibility to use either tool independently\n- Cost optimization by only verifying high-priority claims\n\n## Example Usage\n\n```typescript\nconst result = await factCheckerTool.execute({\n  claim: \"The Great Wall of China is visible from space\",\n  context: \"Common misconception about landmarks\"\n});\n\n// Result includes:\n// - verdict: 'false'\n// - confidence: 'high'\n// - explanation: \"This is a common myth. The Great Wall is not visible...\"\n// - evidence: [\"NASA has confirmed...\", \"Astronauts have stated...\"]\n// - corrections: \"The Great Wall is not visible from space without aid\"\n```\n\n## Integration with FactCheckAnalyzerJob\n\nThe FactCheckAnalyzerJob orchestrates both tools:\n\n1. Extracts claims using extract-factual-claims\n2. Scores and filters claims based on importance, controversiality, and verifiability\n3. Verifies high-priority claims using fact-checker\n4. Generates comments combining extraction scores and verification results",
  "check-math-with-mathjs": "# Check Math with MathJS\n\n*README content not available*",
  "check-math": "# Check Mathematical Accuracy\n\n*README content not available*",
  "check-math-hybrid": "# Hybrid Mathematical Checker\n\n*README content not available*",
  "extract-math-expressions": "# Extract Mathematical Expressions\n\n*README content not available*",
  "extract-forecasting-claims": "# Extract Forecasting Claims Tool\n\nA specialized AI tool for extracting and analyzing forecasting claims from text. Identifies predictions, evaluates their quality, and provides detailed scoring across multiple dimensions.\n\n## Overview\n\nThe Extract Forecasting Claims tool:\n\n1. **Identifies Predictions** - Extracts forecasting statements from text\n2. **Clarifies Intent** - Rewrites vague predictions for better precision\n3. **Scores Quality** - Evaluates predictions across four key dimensions\n4. **Extracts Metadata** - Identifies probabilities, dates, and other key information\n5. **Rates Verifiability** - Assesses how easily predictions can be verified\n\n## Key Features\n\n- **Multi-dimensional Scoring**: Precision, verifiability, importance, and robustness scores\n- **Prediction Clarification**: Rewrites vague predictions for better understanding\n- **Metadata Extraction**: Pulls out resolution dates and probability estimates\n- **Quality Assessment**: Comprehensive evaluation of prediction quality\n- **Batch Processing**: Handles multiple predictions in a single text input\n\n## Scoring Dimensions\n\n### Precision Score (0-100)\n- Measures how specific and well-defined the prediction is\n- Higher scores for predictions with clear timeframes, specific outcomes, and measurable criteria\n- Example: \"Tesla stock will reach $300 by December 2025\" (high precision) vs \"Tesla will do well\" (low precision)\n\n### Verifiability Score (0-100)\n- Assesses how easily the prediction can be verified when resolved\n- Higher scores for predictions with objective, measurable outcomes\n- Example: \"Unemployment will be below 4% in Q1 2025\" (high verifiability) vs \"People will be happier\" (low verifiability)\n\n### Importance Score (0-100)\n- Evaluates the significance and impact of the prediction\n- Higher scores for predictions affecting many people or important decisions\n- Example: \"Global GDP will grow by 3% in 2025\" (high importance) vs \"My local cafÃ© will add a new menu item\" (low importance)\n\n### Robustness Score (0-100)\n- Measures how well-supported and reasonable the prediction appears\n- Higher scores for predictions with clear reasoning or supporting evidence\n- Example: Predictions based on trends, data, or expert analysis score higher\n\n## Score Interpretation\n\n- **70-100**: Excellent quality prediction\n- **40-69**: Moderate quality, may need refinement\n- **0-39**: Poor quality, significant issues present\n\n## Use Cases\n\n### Research Analysis\n```\nExtract and evaluate predictions from research papers, reports, or forecasting documents.\n```\n\n### Content Review\n```\nAssess the quality of predictions in articles, blog posts, or expert commentary.\n```\n\n### Forecast Tracking\n```\nIdentify predictions that are worth tracking and following up on for accuracy.\n```\n\n### Decision Support\n```\nEvaluate the quality of predictions used in strategic planning or decision-making.\n```\n\n## Best Practices\n\n1. **Clear Text Input**: Provide text with well-structured sentences containing predictions\n2. **Review Clarifications**: Check rewritten predictions to ensure they capture the original intent\n3. **Consider Context**: Scores should be interpreted within the domain and context of the prediction\n4. **Track High-Quality Predictions**: Focus follow-up efforts on predictions with high scores\n5. **Validate Extracted Metadata**: Verify that dates and probabilities are correctly extracted\n\n## Integration Workflow\n\nThis tool works well with:\n1. **Perplexity Research Tool** - Research background information for predictions\n2. **Fact Checker Tool** - Verify assumptions underlying the predictions\n3. **Document Analysis Tools** - Process longer documents containing multiple predictions\n\n## Limitations\n\n- May miss implicit or very subtle predictions\n- Scoring is based on text analysis, not domain expertise\n- Cannot verify the accuracy of the predictions themselves\n- May struggle with highly technical or domain-specific predictions\n- Effectiveness varies with writing style and prediction complexity",
  "document-chunker": "# Intelligent Document Chunker\n\n*README content not available*",
  "fuzzy-text-locator": "# Fuzzy Text Locator\n\nA powerful text location tool that finds text within documents using multiple search strategies, from exact matching to AI-powered semantic search.\n\n## Overview\n\nThe Fuzzy Text Locator uses a cascade of search strategies to find text positions in documents:\n\n1. **Exact Match** - Fastest, requires perfect character-for-character match\n2. **Quote-Normalized Match** - Handles smart quotes, apostrophes, and similar variations\n3. **Partial Match** - Finds the longest matching substring for truncated text\n4. **Fuzzy Match (uFuzzy)** - Tolerates typos, case differences, and minor variations\n5. **LLM Fallback** - Uses AI to find paraphrased or semantically similar text\n\n## Key Features\n\n- **Multi-strategy approach**: Tries simple strategies first, falls back to complex ones\n- **Character-level precision**: Returns exact start/end positions in the document\n- **Confidence scoring**: Each strategy provides a confidence score (0.0-1.0)\n- **Quote normalization**: Handles smart quotes, em-dashes, ellipses automatically\n- **Partial matching**: Finds truncated quotes or partial text\n- **Semantic search**: Optional LLM fallback for paraphrased content\n\n## Usage\n\n### Basic Example\n\n```typescript\nimport { findTextLocation } from '@/tools/fuzzy-text-locator';\n\nconst result = await findTextLocation(\n  \"sample document\",\n  \"This is a sample document with some text.\",\n  { normalizeQuotes: true }\n);\n// Returns: { startOffset: 10, endOffset: 25, quotedText: \"sample document\", strategy: \"exact\", confidence: 1.0 }\n```\n\n### Options\n\n```typescript\ninterface TextLocationOptions {\n  // Basic options\n  normalizeQuotes?: boolean;    // Handle quote/apostrophe variations\n  partialMatch?: boolean;       // Match partial/truncated text\n  caseSensitive?: boolean;      // Case-sensitive matching (default: false)\n  \n  // Fuzzy matching options\n  maxTypos?: number;           // Maximum typos allowed in fuzzy search\n  \n  // LLM options\n  useLLMFallback?: boolean;    // Use AI for semantic search\n  llmContext?: string;         // Context to help AI understand\n  pluginName?: string;         // For tracking/logging\n}\n```\n\n## Search Strategies\n\n### Exact Match\n- **Speed**: Fastest\n- **Use case**: When you have the exact text\n- **Confidence**: 1.0\n\n### Quote-Normalized Match\n- **Speed**: Fast\n- **Use case**: Text with smart quotes, apostrophes, em-dashes\n- **Confidence**: 1.0\n- **Example**: \"don't\" matches \"don't\"\n\n### Partial Match\n- **Speed**: Fast\n- **Use case**: Truncated quotes or first part of long text\n- **Confidence**: 0.65-0.7\n- **Example**: \"The research shows\" matches longer quote starting with those words\n\n### Fuzzy Match (uFuzzy)\n- **Speed**: Medium\n- **Use case**: Text with typos, case differences, minor variations\n- **Confidence**: 0.6-0.95\n- **Example**: \"mashine learning\" matches \"machine learning\"\n\n### LLM Fallback\n- **Speed**: Slow (API call)\n- **Use case**: Paraphrased text, semantic similarity\n- **Confidence**: 0.5-0.9\n- **Example**: \"car drove fast\" matches \"automobile moved swiftly\"\n\n## Architecture\n\n```\nfuzzy-text-locator/\nâ”œâ”€â”€ index.ts          # Main tool class and exports\nâ”œâ”€â”€ core.ts           # Core search orchestration logic\nâ”œâ”€â”€ exactSearch.ts    # Simple exact string matching\nâ”œâ”€â”€ uFuzzySearch.ts   # Fuzzy matching with uFuzzy library\nâ”œâ”€â”€ llmSearch.ts      # LLM-based semantic search\nâ”œâ”€â”€ types.ts          # Shared TypeScript types\nâ””â”€â”€ tests/            # Comprehensive test suite\n```\n\n## Testing\n\nThe tool includes a comprehensive test suite with 80+ test cases covering:\n- Basic exact matching\n- Quote and punctuation variations\n- Whitespace handling\n- Unicode characters\n- Partial matches\n- Typos and fuzzy matching\n- Long documents\n- Edge cases\n\nRun tests:\n```bash\nnpm test -- src/tools/fuzzy-text-locator/tests/\n```\n\n## Performance Considerations\n\n- Strategies are tried in order from fastest to slowest\n- Most searches complete in < 10ms without LLM\n- LLM fallback adds 500-2000ms depending on text length\n- For large documents, consider chunking for better performance\n\n## Integration with Plugins\n\nThe tool is designed to be used by analysis plugins:\n\n```typescript\nimport { findTextLocation } from '@/tools/fuzzy-text-locator';\n\n// In your plugin\nconst location = await findTextLocation(\n  errorText,\n  documentText,\n  {\n    normalizeQuotes: true,\n    useLLMFallback: true,\n    pluginName: 'my-plugin'\n  }\n);\n```\n\n## Future Improvements\n\n- [ ] Add caching for repeated searches\n- [ ] Implement parallel search strategies\n- [ ] Add support for regex patterns\n- [ ] Optimize for very large documents\n- [ ] Add more language-specific normalizations",
  "detect-language-convention": "# Detect Language Convention\n\n*README content not available*",
  "forecaster": "# Forecaster Tool\n\nA tool that generates probability forecasts using multiple independent Claude analyses.\n\n## Overview\n\nThe Forecaster Tool asks Claude to make independent probability assessments of a given question, then aggregates them using statistical methods to produce a final forecast with confidence levels.\n\n## Usage\n\n### API Endpoint\n```\nPOST /api/tools/forecaster\n```\n\n### Input Schema\n```typescript\n{\n  question: string;         // The question to forecast (1-500 chars)\n  context?: string;         // Additional context (max 1000 chars)\n  numForecasts?: number;    // Number of forecasts to generate (3-20, default: 6)\n  usePerplexity?: boolean;  // Whether to use Perplexity for research (default: false)\n}\n```\n\n### Output Schema\n```typescript\n{\n  probability: number;      // Aggregated probability (0-100)\n  description: string;      // Description of the forecast and reasoning\n  confidence: 'low' | 'medium' | 'high';  // Based on forecast agreement\n  individualForecasts: Array<{\n    probability: number;\n    reasoning: string;\n  }>;\n  statistics: {\n    mean: number;\n    median: number;\n    stdDev: number;\n    agreement: number;    // % of forecasts within 10 points of median\n  };\n}\n```\n\n## Example\n\n```typescript\nconst response = await fetch('/api/tools/forecaster', {\n  method: 'POST',\n  headers: {\n    'Content-Type': 'application/json',\n    'Authorization': 'Bearer YOUR_TOKEN'\n  },\n  body: JSON.stringify({\n    question: \"Will AGI be achieved by 2030?\",\n    context: \"Recent advances in LLMs have accelerated AI progress\",\n    numForecasts: 6\n  })\n});\n\nconst result = await response.json();\n// {\n//   success: true,\n//   toolId: 'forecaster',\n//   result: {\n//     probability: 35,\n//     description: \"Based on 6 independent analyses...\",\n//     confidence: 'medium',\n//     individualForecasts: [...],\n//     statistics: {...}\n//   }\n// }\n```\n\n## Cost\n\nApproximately $0.05 per forecast (6 Claude calls).\n\n## Implementation Details\n\n- Uses multiple independent Claude calls to reduce bias\n- Removes statistical outliers before aggregation\n- Confidence levels based on forecast agreement:\n  - High: >66% of forecasts within 10 points of median\n  - Medium: 33-66% agreement\n  - Low: <33% agreement",
  "link-validator": "# Link Validator Tool\n\n## What This Tool Does\n\nThe Link Validator tool extracts and validates all external links in documents, providing detailed accessibility status and error categorization. It offers comprehensive link health analysis without using any LLM calls.\n\nAlso known as the \"Simple Link Validator\" agent, this tool checks all external links in documents and reports their accessibility status with clear error categorization and context-aware messaging about link health.\n\n## Core Capabilities\n\n- **Automatic Link Detection**: Finds all URLs in document content (markdown links, HTML links, plain URLs)\n- **Multi-Strategy Validation**: Tests HEAD requests first, falls back to GET requests with different user agents\n- **Smart Error Classification**: Categorizes failures as 403 Forbidden, 404 Not Found, timeouts, network errors, etc.\n- **Highlight Generation**: Creates positioned comments for broken links in documents\n- **Context-Aware Reporting**: Adapts messaging based on predominant error types\n\n## Usage\n\n```typescript\nimport { linkValidator } from '@roast/ai/server';\n\n// Basic usage\nconst result = await linkValidator.run({\n  text: documentContent,\n  maxUrls: 20  // optional, defaults to 20\n}, {\n  logger: console\n});\n\n// Result includes:\n// - urls: string[] - all URLs found\n// - validations: detailed validation results for each URL\n// - summary: statistics about link health\n```\n\n## Output Structure\n\n### Summary Statistics\n```typescript\n{\n  totalLinks: number,\n  workingLinks: number,\n  brokenLinks: number,\n  errorBreakdown: {\n    NotFound: number,\n    Forbidden: number,\n    Timeout: number,\n    NetworkError: number,\n    // ... other error types\n  }\n}\n```\n\n### Individual Link Validations\nEach URL validation includes:\n- `url`: Original URL\n- `finalUrl`: URL after redirects (if different)\n- `accessible`: Boolean indicating if link works\n- `error`: Error details if link failed\n- `details`: Response details if link succeeded\n\n## Error Types\n\n- **NotFound** (404): Page doesn't exist\n- **Forbidden** (403): Access denied\n- **Timeout**: Request timed out\n- **NetworkError**: DNS, SSL, or connection errors\n- **RateLimited** (429): Too many requests\n- **ServerError** (5xx): Server-side errors\n- **Unknown**: Other HTTP errors\n\n## Advanced Features\n\n### Highlight Generation\nGenerate document highlights for link issues:\n\n```typescript\nimport { generateLinkHighlights } from '@roast/ai/tools/link-validator/linkHighlightGenerator';\n\nconst highlights = generateLinkHighlights(\n  validationResults,\n  urls,\n  documentContent,\n  targetHighlights // optional limit\n);\n```\n\n### Analysis Report Generation\nCreate detailed analysis reports:\n\n```typescript\nimport { generateLinkAnalysisAndSummary } from '@roast/ai/tools/link-validator/linkAnalysisReporter';\n\nconst { analysis, summary, grade } = generateLinkAnalysisAndSummary(\n  validationResults,\n  documentTitle\n);\n```\n\n## Example Messages\n\n**Access-restriction focused** (when most errors are 403s):\n> ðŸš« Links Blocked by Access Restrictions  \n> Found 8 inaccessible URLs, primarily due to access restrictions. Many websites block automated access, even though the content exists.\n\n**Broken-links focused** (when most errors are 404s):\n> âŒ Broken Links Detected  \n> Found 6 broken or non-existent URLs. These may be hallucinated links or references to content that has moved or been deleted.\n\n## When to Use\n\n**Ideal for**: \n- Research papers with citations\n- Blog posts with external references\n- News articles\n- Documentation with external links\n- Any content where link quality affects credibility\n\n**Less suitable for**: \n- Documents without external links\n- Creative writing\n- Internal documentation with private URLs\n\n## Technical Notes\n\n- Uses respectful validation with delays between requests\n- Tries multiple user agents to handle basic bot detection  \n- 10-second timeout per request for performance\n- Processes URLs in batches of 10 to avoid overwhelming servers\n- No prescriptive recommendations - focuses on status reporting only\n- Zero LLM usage - all validation is done directly",
  "perplexity-research": "# Perplexity Research Tool\n\nA research assistant tool that uses the Perplexity API to search for up-to-date information on any topic. Perfect for gathering context, finding recent developments, and collecting sources for analysis and forecasting tasks.\n\n## Overview\n\nThe Perplexity Research Tool leverages Perplexity AI's search capabilities to:\n\n1. **Search across the web** - Accesses current information from multiple sources\n2. **Summarize findings** - Provides concise summaries of research results  \n3. **Categorize sources** - Ranks sources by relevance (high/medium/low)\n4. **Extract key findings** - Highlights the most important insights\n5. **Support focus areas** - Tailors searches to specific domains (academic, news, technical, etc.)\n\n## Key Features\n\n- **Real-time search**: Accesses current information, not limited to training data\n- **Source categorization**: Automatically ranks sources by relevance\n- **Configurable results**: Control number of sources (3-10)\n- **Focus area targeting**: Optimize searches for different domains\n- **Key findings extraction**: Automatically identifies important insights\n- **Source metadata**: Full URLs, titles, and snippets for verification\n\n## Usage\n\n### Basic Research Query\n\n```typescript\nimport { perplexityResearchTool } from '@roast/ai';\n\nconst result = await perplexityResearchTool.execute({\n  query: \"What are the latest developments in quantum computing error correction?\",\n  maxSources: 5,\n  focusArea: \"academic\"\n});\n```\n\n### Focus Areas\n\n- **`general`** - Broad web search across all sources\n- **`academic`** - Prioritize scholarly articles and research papers\n- **`news`** - Focus on recent news and current events\n- **`technical`** - Emphasize technical documentation and specifications\n- **`market`** - Target financial and market information\n\n### Response Format\n\n```typescript\ninterface ResearchResult {\n  query: string;\n  summary: string;\n  sources: Array<{\n    title: string;\n    url: string;\n    snippet: string;\n    relevance: 'high' | 'medium' | 'low';\n  }>;\n  keyFindings: string[];\n  timestamp: string;\n}\n```\n\n## Integration with Analysis Workflows\n\nThis tool is particularly valuable for:\n\n### Forecasting Tasks\n- Gather recent developments on prediction topics\n- Find expert opinions and analysis\n- Collect baseline facts for probability assessments\n\n### Fact-Checking\n- Verify claims against current sources\n- Find supporting or contradicting evidence\n- Access recent updates to evolving topics\n\n### Context Building\n- Research background information for document analysis\n- Find related developments and trends\n- Gather expert perspectives\n\n## Example Queries\n\n### Technology Research\n```\n\"Latest breakthroughs in large language model efficiency improvements\"\n```\n\n### Market Analysis\n```  \n\"Current trends in renewable energy investment and policy changes 2024\"\n```\n\n### Scientific Updates\n```\n\"Recent advances in CRISPR gene editing safety and regulations\"\n```\n\n## Cost and Performance\n\n- **Response time**: 2-5 seconds depending on query complexity\n- **Source diversity**: Typically returns 5-10 high-quality sources\n- **Update frequency**: Accesses information updated within hours or days\n- **Rate limits**: Managed automatically with backoff strategies\n\n## Best Practices\n\n1. **Be specific**: More specific queries yield better, more relevant results\n2. **Use focus areas**: Choose the appropriate focus area for your research domain\n3. **Verify sources**: Always check the provided URLs for full context\n4. **Combine with other tools**: Use results to inform fact-checking or forecasting analyses\n5. **Track timestamps**: Note when research was conducted for time-sensitive topics\n\n## Limitations\n\n- Dependent on Perplexity API availability and rate limits\n- Results quality varies with query specificity\n- May not access paywalled or restricted content\n- Focus areas are suggestions, not strict filters"
} as const;

export type ToolId = keyof typeof toolReadmes;

export function getToolReadme(toolId: string): string {
  const typedToolId = toolId as ToolId;
  return toolReadmes[typedToolId] || `# ${toolId}\n\n*README content not available*`;
}
