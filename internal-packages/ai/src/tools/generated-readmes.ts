/**
 * Auto-generated README content
 * Generated by scripts/generate-readmes.ts
 * DO NOT EDIT MANUALLY
 *
 * README Hash: c3ffbecc11db9a81364f2075fc583d9ec9c05025b6ef01ddbd1ebf0ea24d3e88
 */

export const toolReadmes = {
  "spelling-grammar-checker": "# Spelling & Grammar Checker\n\nAnalyze text for spelling and grammar errors using Claude with advanced error detection\n\n## Tools Used\n\n- **[English Dialect Detector](/tools/language-convention-detector)** - Detect whether text uses US or UK English conventions\n\n## How It Works\n\nFirst detects the document's language convention (US/UK/mixed) using the detect-language-convention tool, then sends text to Claude with detailed instructions for error detection. The tool uses importance scoring and confidence levels to prioritize errors, with configurable strictness levels (minimal/standard/thorough) that adjust the error detection threshold.\n\n## Example\n\n**Input:** \"The team of engineers are working on the project.\"\n\n**Output:**\n```json\n{\n  \"text\": \"are\",\n  \"correction\": \"is\", \n  \"displayCorrection\": \"<r:replace from=\\\"are\\\" to=\\\"is\\\"/>\",\n  \"type\": \"grammar\",\n  \"context\": \"engineers are working on\",\n  \"importance\": 45,\n  \"confidence\": 85,\n  \"description\": \"The subject 'team' is singular and requires the singular verb 'is', not the plural 'are'.\",\n  \"lineNumber\": 1\n}\n```\n\n## Limitations\n\nUses Claude Sonnet 4.5 model. Limited to `50` errors by default (configurable). Line numbers are approximate. For exact position finding, combine with smart-text-searcher tool.\n\n## Technical Details\n\n- **Strictness levels:** minimal (importance ≥51), standard (≥26), thorough (≥0)\n- **Convention modes:** US, UK, or auto-detect with mixed convention support\n- **Error scoring:** importance (0-100), confidence (0-100), with contextual descriptions\n",
  "factual-claims-extractor": "# Factual Claims Extractor\n\nExtract and score verifiable factual claims from text\n\n## Tools Used\n\n- **[Smart Text Searcher](/tools/smart-text-searcher)** - Find the location of text within documents using multiple search strategies including exact matching, fuzzy matching, quote normalization, partial matching, and LLM fallback for paraphrased or difficult-to-find text\n\n## How It Works\n\nAnalyzes text to identify factual statements, then scores each claim across three dimensions: importance (how central to the argument), checkability (how easily fact-checked), and truth probability (estimated likelihood of being true). Assigns topic categories and confidence scores for each extraction.\n\n## Limitations\n\nMay occasionally extract opinions as factual claims. Can miss context-dependent or implicit claims. Performance varies by domain and document structure.\n\n## Integration\n\nWorks well with:\n- **Fact Checker Tool**: Verify extracted claims for accuracy\n- **Perplexity Research Tool**: Find sources for claim verification\n\n## Technical Details\n\n- Extracts multiple claims in single operation\n- Scoring dimensions: importance, checkability, truth probability (0-100 each)\n- Topic categorization (economics, history, science, etc.)\n- Uses smart-text-searcher for precise text positioning\n",
  "fact-checker": "# Fact Checker\n\nVerify the accuracy of specific factual claims\n\n## Tools Used\n\n- **[Perplexity Researcher](/tools/perplexity-researcher)** - Web-enhanced research using Perplexity Sonar models via OpenRouter\n\n## How It Works\n\nUses Perplexity Research to gather current information and sources, then analyzes specific claims for accuracy and truthfulness. Returns structured verdicts (true, false, partially-true, unverifiable, or outdated) with detailed reasoning and supporting evidence.\n\n## Limitations\n\nEffectiveness depends on claim specificity and available evidence sources. Cannot verify highly specialized or very recent claims.\n\n## Integration\n\nWorks with **Extract Factual Claims** tool:\n1. Extract claims from documents\n2. Prioritize high-importance claims\n3. Verify selected claims for accuracy\n4. Generate comprehensive fact-check reports\n\n## Technical Details\n\n- Uses Perplexity Research tool for web-enhanced information gathering\n- Provides structured verdict types: true, false, partially-true, unverifiable, outdated\n- Includes confidence scoring and correction suggestions\n- Best used for high-priority or controversial claims\n",
  "math-validator-mathjs": "# Math Validator (MathJS)\n\nVerify mathematical statements using an agentic approach with Claude and MathJS\n\n## How It Works\n\nThe tool uses a mathematical calculator ([MathJS](https://mathjs.org/)) to verify numerical statements and calculations. It can handle arithmetic, algebra, trigonometry, unit conversions, and more.\n\n## Limitations\n\nCannot handle symbolic mathematics (derivatives, integrals, proofs). Limited to numerical computations and basic algebra. Uses Claude Sonnet 4.5 model for agent mode. May timeout on very complex calculations (60-second limit).\n\n## Two-Mode Design\n\nThe tool intelligently chooses between two approaches:\n\n**Direct Mode (Fast & Free):** For simple equality statements like \"2 + 2 = 4\", the tool calculates directly using MathJS without needing Claude. This is instant and costs nothing.\n\n**Agent Mode (Smart & Flexible):** For complex statements, word problems, or multi-step calculations, Claude gets access to two tools:\n- `evaluate_expression` - Uses MathJS to calculate any mathematical expression\n- `provide_verdict` - Gives the final true/false judgment with explanation\n\nThis hybrid approach gives you the speed of direct calculation for simple cases while providing the reasoning power of Claude for complex problems.\n\n## Smart Approximation Handling\n\nThe tool intelligently handles mathematical approximations based on the precision you show:\n\n**Examples of what gets accepted:**\n- \"10/3 = 3.33\" ✅ (3.333... rounds to 3.33)\n- \"π = 3.14\" ✅ (3.14159... rounds to 3.14) \n- \"√2 = 1.414\" ✅ (1.41421... rounds to 1.414)\n\n**Examples of what gets rejected:**\n- \"π = 3.0\" ❌ (3.14159... rounds to 3.1, not 3.0)\n- \"10/3 = 3.0\" ❌ (3.333... rounds to 3.3, not 3.0)\n\n## Technical Details\n\n- Two-mode operation: Direct (simple equality) vs Agent (complex statements)\n- Uses Claude Sonnet 4.5 model for agent mode\n- MathJS calculator for numerical computations\n- Smart approximation handling based on precision shown\n- Timeout: 60 seconds for complex calculations\n",
  "math-validator-llm": "# Math Validator (LLM)\n\nAnalyze text for mathematical errors including calculations, logic, units, and notation using Claude\n\n## How It Works\n\nThe tool sends mathematical statements directly to Claude for analysis. Claude evaluates the statement and returns a structured response indicating whether it's true, false, or cannot be verified, along with detailed reasoning and error categorization when applicable.\n\n## Limitations\n\nMay make arithmetic errors on complex calculations since it doesn't use a calculator. Non-deterministic - the same input might produce slightly different explanations. Uses Claude Sonnet 4.5 model.\n\n## Technical Details\n\n- Uses Claude Sonnet 4.5 model for analysis\n- Response format: status (verified_true/verified_false/cannot_verify), explanation, reasoning, error details\n- Error categorization: calculation, logic, unit, notation, conceptual with severity levels\n- Uses deterministic cache seeds for more consistent responses\n",
  "math-validator-hybrid": "# Math Validator (Hybrid)\n\nSimple wrapper: try MathJS first, then LLM as fallback\n\n## Tools Used\n\n- **[Math Validator (MathJS)](/tools/math-validator-mathjs)** - Verify mathematical statements using an agentic approach with Claude and MathJS\n- **[Math Validator (LLM)](/tools/math-validator-llm)** - Analyze text for mathematical errors including calculations, logic, units, and notation using Claude\n\n## How It Works\n\nAttempts numerical verification using MathJS Agent first (fast, accurate for numerical calculations). If MathJS returns `cannot_verify` (e.g., for symbolic math like derivatives), falls back to pure Claude analysis for conceptual mathematics.\n\n## Limitations\n\nCannot perform real-time symbolic algebra or step-by-step equation solving. MathJS requires numerical values - can't simplify \"x + x = 2x\" symbolically. Uses Claude Sonnet 4.5 model.\n\n## Technical Details\n\n- Two-stage verification: MathJS Agent → LLM Fallback (if needed)\n- Uses Claude Sonnet 4.5 model for both stages\n- 60-second timeout protection\n- Returns which tool verified (mathjs/llm) with explanation and optional corrections\n",
  "math-expressions-extractor": "# Math Expressions Extractor\n\nExtract and analyze mathematical expressions from text, including error detection and complexity assessment\n\n## How It Works\n\nUses Claude to identify mathematical expressions that appear potentially incorrect, focusing on arithmetic errors, wrong calculations, and unit conversion mistakes. Deliberately filters out correct expressions and factual/forecasting claims handled by other tools. Returns expressions with error likelihood scores and surrounding context.\n\n**Limitations:** Intentionally restrictive - won't extract correct math or simple expressions. Cannot verify correctness itself (use math checker tools for that). May miss subtle errors in complex formulas.\n\n## Technical Details\n\n- Extraction threshold: Only expressions with `20%`+ error likelihood\n- Exclusions: Correct math, simple percentages, factual claims, predictions\n- Output: Expression text, error likelihood, context, character offsets\n- Uses Claude to identify potentially incorrect expressions\n",
  "binary-forecasting-claims-extractor": "# Binary Forecasting Claims Extractor\n\nExtracts predictions and converts them to binary (YES/NO) questions. Scores on four dimensions: precision (how binary/specific), verifiability (can we check with public data), importance (centrality to argument), and robustness (how well-supported)\n\n## How It Works\n\nExtracts forecasting statements from text and converts them to binary (YES/NO) questions. Clarifies vague predictions for better precision. Scores each prediction across four dimensions: precision (how specific), verifiability (can we check with public data), importance (centrality to argument), and robustness (how well-supported).\n\n## Limitations\n\nMay miss subtle predictions. Effectiveness varies with writing style and domain complexity. Scoring based on text analysis, not domain expertise. Cannot verify prediction accuracy (only evaluates quality).\n\n## Technical Details\n\n- Scoring dimensions: precision, verifiability, importance, robustness (0-100 each)\n- Score interpretation: 70-100 (excellent), 40-69 (moderate), 0-39 (poor)\n- Clarifies predictions to ensure they are specific and binary\n- Extracts probabilities, dates, and resolution criteria when present\n",
  "document-chunker": "# Document Chunker\n\nSplits documents into semantic chunks optimized for LLM analysis. Supports multiple strategies including markdown-aware, semantic, and hybrid chunking.\n\n## How It Works\n\nParses markdown hierarchy to identify sections and headings, then recursively chunks content based on target word count (default 500 words). Handles code blocks, lists, and nested sections intelligently. Automatically merges small gaps between chunks (≤5 chars) and ensures no text is lost by creating filler chunks for larger gaps. Maintains heading context for each chunk.\n\n## Limitations\n\nCurrently only uses markdown strategy despite having semantic and hybrid methods in code. May not work well for non-markdown text. Target word count is approximate - actual chunks may vary. Cannot handle extremely nested structures beyond reasonable depth.\n\n## Technical Details\n\n- **Strategy:** Always uses markdown-aware chunking (ignores strategy parameter)\n- **Default target:** 500 words per chunk (configurable via targetWords)\n- **Character limits:** maxChunkSize (default 1500), minChunkSize (default 200)\n- **Output:** Chunks with offsets, line numbers, type metadata, and heading context\n",
  "smart-text-searcher": "# Smart Text Searcher\n\nFind the location of text within documents using multiple search strategies including exact matching, fuzzy matching, quote normalization, partial matching, and LLM fallback for paraphrased or difficult-to-find text\n\n## How It Works\n\nUses cascading search strategies to find text within documents, trying faster methods first (exact match, quote normalization) before falling back to slower methods (fuzzy matching, LLM semantic search). Each match returns exact character positions with confidence scores (0.0-1.0).\n\n## Limitations\n\nPerformance decreases with document size. LLM fallback requires API access and adds 500-2000ms. Cannot handle severe paraphrasing without LLM fallback enabled.\n\n## Technical Details\n\n- Strategy cascade: exact → quote-normalized → (partial | fuzzy | markdown-aware) → LLM\n- Confidence scoring: 0.0-1.0 scale\n- Configuration options: normalizeQuotes, partialMatch, maxTypos, useLLMFallback\n- Primary use case: Precise text positioning for analysis plugin annotations\n",
  "language-convention-detector": "# English Dialect Detector\n\nDetect whether text uses US or UK English conventions\n\n## How It Works\n\nAnalyzes text against comprehensive US/UK word pair dictionaries (500+ word variations including -ize/-ise, -or/-our, -er/-re patterns). Applies pattern-based weighting (e.g., -ize/-ise differences weighted higher than -or/-our) and word frequency weights for common terms. Also detects document type (academic, technical, blog, casual) for additional context.\n\n## Limitations\n\nFrequently fails to detect the correct language convention. Cannot detect Australian, Canadian, or other English variants. Requires sufficient distinctive words for accurate detection. \n\n## Technical Details\n\n- **Dictionary size:** 500+ US/UK word pairs with variations\n- **Pattern weights:** Different patterns weighted by distinctiveness\n- **Output:** Convention (US/UK), confidence (0-1), consistency (0-1), evidence array, document type\n",
  "binary-forecaster": "# Binary Forecaster\n\nGenerate probability forecasts using multiple independent Claude analyses\n\n## How It Works\n\nAsks Claude to make multiple independent probability assessments of a given question, then aggregates them by taking the mean to produce a final forecast.\n\n## Output\n\n- **probability**: Aggregated probability (0-100)\n- **description**: Description of the forecast and reasoning\n- **confidence**: 'low' | 'medium' | 'high' based on forecast agreement\n- **individualForecasts**: Array of individual probability assessments with reasoning\n- **statistics**: Mean, median, standard deviation, and agreement metrics\n\n## Confidence Levels\n\n- **High**: >66% of forecasts within 10 points of median\n- **Medium**: 33-66% agreement\n- **Low**: <33% agreement\n\n## Technical Details\n\n- Uses multiple independent Claude Sonnet 4.5 calls to reduce bias\n- Removes statistical outliers before aggregation\n- Agreement measured as percentage of forecasts within 10 points of median\n- Default 6 forecasts per question (configurable 3-20)\n",
  "link-validator": "# Link Validator\n\nExtracts and validates all URLs from a text, checking their accessibility and returning detailed validation results\n\n## How It Works\n\nAutomatically detects all URLs in text (markdown, HTML, plain text) and validates their accessibility using multi-strategy approach: GraphQL APIs for LessWrong and EA Forum posts, HTTP requests (HEAD then GET with different user agents) for other sites. Categorizes each link as working, broken, or blocked with detailed error types.\n\n## Limitations\n\nMany websites block automated access even when content exists. Cannot access paywalled content. Best used as first pass before human review. 403 errors often indicate bot detection, not broken content.\n",
  "perplexity-researcher": "# Perplexity Researcher\n\nWeb-enhanced research using Perplexity Sonar models via OpenRouter\n\n## How It Works\n\nUses Perplexity's Sonar models via OpenRouter to perform web-enhanced research. Searches current information beyond AI training data and returns structured results with summaries, key findings, and source citations (3-10 sources with titles, URLs, snippets). Includes fallback mechanisms for reliability.\n\n## Limitations\n\nDepends on Perplexity API and OpenRouter availability. Quality varies by query complexity and available sources. Cannot access paywalled or restricted content.\n\n## Technical Details\n\n- Uses Perplexity Sonar models via OpenRouter API\n- Configurable focus areas: general, academic, news, technical, market\n- Returns 3-10 sources per query (configurable)\n- Fallback: structured research → basic query with parsing\n- Primary use: Supporting fact-checking and forecasting tools\n",
  "claim-evaluator": "# Claim Evaluator\n\nEvaluate claims by polling multiple LLM models in parallel (Claude, GPT, Grok) via OpenRouter. Each model provides an agreement score (0-100), brief reasoning, and response time in milliseconds.\n\n## How It Works\n\nEvaluates claims by polling multiple LLM models in parallel via OpenRouter. Each model independently rates its agreement with the claim (0-100) and provides brief reasoning.\n\n## Default Models\n\n- **anthropic/claude-sonnet-4.5** - Claude Sonnet 4.5 (Latest)\n- **openai/gpt-5-mini** - GPT-5 Mini\n- **deepseek/deepseek-chat-v3.1** - DeepSeek Chat V3.1\n- **x-ai/grok-4** - Grok 4\n\n## Output\n\n> **Note**: Each evaluation has a `hasError` boolean that determines its structure. When `hasError` is false, `successfulResponse` is present. When `hasError` is true, `failedResponse` is present.\n\n- **evaluations**: Array of all model evaluations (both successful and failed). Each evaluation has these common fields:\n  - `hasError`: Boolean indicating if evaluation failed (true) or succeeded (false)\n  - `model`: Model identifier (e.g., \"anthropic/claude-3-haiku\")\n  - `provider`: Provider name (e.g., \"anthropic\", \"openai\")\n  - `responseTimeMs`: Time taken for LLM to respond (optional)\n  - `rawResponse`: Full raw response from model (optional)\n  - `thinkingText`: Extended thinking/reasoning for o1/o3 models (optional)\n  - `tokenUsage`: Token usage statistics (optional)\n\n**For successful evaluations (hasError: false):**\n  - `successfulResponse`: Object containing:\n    - `agreement`: Score from 0-100 (0=disagree, 100=agree)\n    - `confidence`: Score from 0-100 (0=very uncertain, 100=very confident)\n    - `reasoning`: Brief explanation (max words configurable, default 50)\n\n**For failed evaluations (hasError: true):**\n  - `failedResponse`: Object containing:\n    - `error`: Error message\n    - `refusalReason`: Categorized reason ('Safety', 'Policy', 'MissingData', 'Unclear', 'Error')\n    - `errorDetails`: Additional error context (optional)\n\n- **summary** (optional): Aggregated statistics across all evaluations\n  - `mean`: Mean agreement score across all successful evaluations\n\n## Technical Details\n\n- All requests go through **OpenRouter** (not direct provider APIs)\n- Helicone integration for request tracking and caching\n- Parallel execution using `Promise.allSettled()`\n- Both successful and failed evaluations are included in results\n- Structured JSON responses with `hasError` boolean and optional response fields\n- Custom models can be specified via the `models` parameter\n- Response times tracked for performance monitoring\n\n## Use Cases\n\n- **Fact-checking**: Get multi-model consensus on factual claims\n- **Prediction evaluation**: Assess plausibility of forecasts\n- **Claim validation**: Identify controversial or uncertain statements\n- **Research**: Compare model perspectives on complex topics\n\n## Example\n\n**Input:**\n```json\n{\n  \"claim\": \"The US economy will grow by 40% in the next 5 years\"\n}\n```\n\n**Output:**\n```json\n{\n  \"evaluations\": [\n    {\n      \"hasError\": false,\n      \"model\": \"anthropic/claude-3-haiku\",\n      \"provider\": \"anthropic\",\n      \"responseTimeMs\": 1234,\n      \"successfulResponse\": {\n        \"agreement\": 15,\n        \"confidence\": 85,\n        \"reasoning\": \"Unlikely growth rate given historical economic data\"\n      }\n    },\n    {\n      \"hasError\": false,\n      \"model\": \"anthropic/claude-3-5-sonnet\",\n      \"provider\": \"anthropic\",\n      \"responseTimeMs\": 2156,\n      \"successfulResponse\": {\n        \"agreement\": 20,\n        \"confidence\": 90,\n        \"reasoning\": \"Historically implausible; requires 7% annual growth\"\n      }\n    },\n    {\n      \"hasError\": true,\n      \"model\": \"some-unavailable-model\",\n      \"provider\": \"openai\",\n      \"failedResponse\": {\n        \"error\": \"Model evaluation timed out after 120s\",\n        \"refusalReason\": \"Error\"\n      }\n    }\n    // ... other models\n  ],\n  \"summary\": {\n    \"mean\": 17.5\n  }\n}\n```\n"
} as const;

export type ToolId = keyof typeof toolReadmes;

export function getToolReadme(id: string): string {
  const typedId = id as ToolId;
  return toolReadmes[typedId] || `# ${id}\n\n*README content not available*`;
}
