/**
 * Auto-generated plugin README content
 * Generated by scripts/generate-plugin-readmes.ts
 * DO NOT EDIT MANUALLY
 *
 * README Hash: 468e6dce7ae0c8ff11a910c4160d2e98dfe61cfd4b60009f1c996e7b2806330a
 */

export const pluginReadmes = {
  "fact-checker": "# Fact Checker\n\nAn agent that identifies and verifies factual claims in documents, checking them against current knowledge and reliable sources. Provides detailed verdicts on claim accuracy with evidence-based reasoning.\n\n## Configuration\n\n**Processing Limits:**\n- Maximum facts to process: **30**\n- Maximum claims per chunk: **20**\n\n**Quality Thresholds:**\n- Minimum quality threshold: **25**\n\n**Importance Scoring:**\n- High importance: **60+**\n- Medium importance: **40+**\n\n**Checkability Scoring:**\n- High checkability: **50+**\n\n**Truth Probability Ranges:**\n- High: **90+**\n- Medium: **70-90**\n- Low: **50-70**\n- Very Low: **≤40**\n- Likely False: **≤30**\n\n## How It Works\n\nThe agent processes documents by:\n1. Extracting factual claims and statements\n2. Categorizing claims by type (statistical, historical, scientific, etc.)\n3. Verifying each claim against current knowledge\n4. Providing verdicts with supporting evidence\n5. Suggesting corrections for inaccurate claims\n\n## Verification Categories\n\n- **Factual claims**: General statements about events, people, or things\n- **Statistical data**: Numbers, percentages, measurements, trends\n- **Historical facts**: Dates, events, historical figures and contexts\n- **Scientific facts**: Research findings, natural phenomena, technical data\n- **Geographic facts**: Locations, distances, demographic information\n- **Current events**: Recent developments, ongoing situations\n\n## Verdict Types\n\n- **True**: Claim is accurate and well-supported\n- **False**: Claim is demonstrably incorrect\n- **Partially True**: Contains accurate elements but is misleading or incomplete\n- **Outdated**: Was true but no longer current\n- **Unverifiable**: Cannot be verified with available information\n- **Misleading**: Technically true but presented in a deceptive way\n\n---\n*This documentation is programmatically generated from source code. Do not edit manually.*\n",
  "math-checker": "# Math Checker\n\nAn agent that verifies mathematical statements, calculations, and formulas for accuracy. Combines computational verification with conceptual analysis to catch errors in arithmetic, algebra, statistics, and mathematical reasoning.\n\n## How It Works\n\nThe agent analyzes mathematical content in documents by:\n1. Extracting mathematical expressions and statements\n2. Verifying calculations using both computational tools and mathematical reasoning\n3. Checking unit consistency and dimensional analysis\n4. Validating statistical claims and data interpretations\n5. Identifying conceptual errors in mathematical logic\n\n## Capabilities\n\n- **Arithmetic verification** - Basic calculations, percentages, ratios\n- **Algebraic checking** - Equation solving, simplification, factoring\n- **Statistical validation** - Means, medians, correlations, significance tests\n- **Unit analysis** - Dimensional consistency, conversion errors\n- **Formula verification** - Scientific formulas, financial calculations\n- **Conceptual review** - Mathematical logic, proof steps, reasoning errors\n\n## Error Categories\n\n- **Calculation errors**: Incorrect arithmetic or computational mistakes\n- **Unit errors**: Dimensional inconsistencies or conversion mistakes\n- **Logic errors**: Flawed mathematical reasoning or invalid steps\n- **Notation errors**: Incorrect mathematical symbols or expressions\n- **Conceptual errors**: Misunderstanding of mathematical principles\n\n---\n*This documentation is programmatically generated from source code. Do not edit manually.*\n",
  "spelling-grammar": "# Spelling & Grammar Checker\n\nA sophisticated proofreading agent that combines language convention detection with Claude-based error analysis. Features adjustable strictness levels and automatic US/UK English convention handling.\n\n## Configuration\n\n**Error Processing:**\n- Maximum errors per chunk: **20**\n- Minimum confidence threshold: **30**\n- High importance threshold: **50**\n\n**Language Convention Detection:**\n- Sample size for detection: **2000 characters**\n- Low confidence threshold: **0.8**\n- Low consistency threshold: **0.8**\n\n## How It Works\n\nFirst detects the document's language convention (US/UK/mixed) using convention detection, then analyzes text with Claude for error detection. The agent uses importance scoring (0-100) and confidence levels to prioritize errors, with configurable strictness levels (minimal/standard/thorough) that adjust the error detection threshold.\n\n## Capabilities\n\n- **Intelligent convention handling** - Can enforce specific US/UK spelling or adapt to mixed conventions\n- **Three strictness levels** for different use cases (minimal, standard, thorough)\n- **Returns exact error text** with concise corrections, importance scores, and confidence ratings\n- **Provides explanations** only for complex errors to reduce noise\n- **Line number tracking** for approximate error locations\n\n## Technical Details\n\n- **Strictness levels:** minimal (importance ≥51), standard (≥26), thorough (≥0)\n- **Convention modes:** US, UK, or auto-detect with mixed convention support\n- **Error scoring:** importance (0-100), confidence (0-100), with contextual descriptions\n- **Maximum errors:** 50 by default (configurable)\n\n---\n*This documentation is programmatically generated from source code. Do not edit manually.*\n",
  "forecast-checker": "# Forecast Checker\n\nAn agent that evaluates predictions, forecasts, and future-oriented claims for methodological soundness, evidence quality, and logical consistency. Assesses both quantitative and qualitative forecasting approaches.\n\n## How It Works\n\nThe agent analyzes forecasting content by:\n1. Identifying predictions and forward-looking statements\n2. Evaluating the evidence and reasoning supporting forecasts\n3. Assessing methodological approaches and assumptions\n4. Checking for logical consistency and bias\n5. Reviewing uncertainty quantification and confidence intervals\n\n## Capabilities\n\n- **Prediction extraction** - Identifies explicit and implicit forecasts\n- **Method evaluation** - Assesses forecasting models and approaches\n- **Evidence analysis** - Reviews supporting data and assumptions\n- **Uncertainty assessment** - Evaluates confidence levels and ranges\n- **Bias detection** - Identifies overconfidence and systematic errors\n- **Logical consistency** - Checks internal coherence of predictions\n\n## Analysis Categories\n\n- **Base rate neglect**: Ignoring historical frequencies\n- **Anchoring bias**: Over-reliance on initial estimates\n- **Overconfidence**: Unrealistic precision or certainty\n- **Poor calibration**: Misaligned confidence and accuracy\n- **Insufficient evidence**: Predictions without adequate support\n- **Methodology flaws**: Inappropriate forecasting techniques\n\n---\n*This documentation is programmatically generated from source code. Do not edit manually.*\n",
  "link-checker": "# Link Checker\n\nAn automated link validation agent that checks all external URLs in documents for accessibility, validity, and potential issues. Provides detailed reporting on broken links, redirects, and connection problems.\n\n## How It Works\n\nExtracts all external URLs from the document and validates each one by checking HTTP status codes and following redirects. The agent identifies broken links (404s), server errors (5xx), excessive redirects, and connection timeouts. Results are provided with exact text locations for easy correction.\n\n## Capabilities\n\n- **Comprehensive URL extraction** - Finds all external links in markdown, HTML, and plain text formats\n- **Status code validation** - Checks HTTP/HTTPS responses and categorizes issues\n- **Redirect chain analysis** - Follows and reports redirect chains up to reasonable limits\n- **Timeout handling** - Gracefully handles slow or unresponsive servers\n- **Bulk processing** - Efficiently validates multiple URLs with parallel checking\n- **Detailed reporting** - Provides exact link text and surrounding context for each issue\n\n## Technical Details\n\n- **URL limit:** 50 links by default (configurable)\n- **Timeout:** 10 seconds per URL check\n- **Redirect limit:** Maximum 5 redirects followed\n- **Supported protocols:** HTTP, HTTPS\n- **Grade calculation:** Based on percentage of working links (100% = grade 100)\n- **Error categories:** 404 Not Found, 5xx Server Errors, Connection Timeouts, Too Many Redirects\n\n---\n*This documentation is programmatically generated from source code. Do not edit manually.*\n",
  "comprehensive-checker": "# Comprehensive Checker\n\nA thorough content validation agent that combines all verification tools except Spelling & Grammar checking. Runs Link Checker, Fact Checker, Math Checker, and Forecast Checker in parallel to provide complete content verification in a single pass.\n\n## How It Works\n\nThis agent performs multi-domain verification by running four specialized checkers simultaneously:\n1. **Link Checker**: Validates all external URLs for accessibility and validity\n2. **Fact Checker**: Verifies factual claims against current knowledge\n3. **Math Checker**: Checks calculations, formulas, and quantitative reasoning\n4. **Forecast Checker**: Evaluates predictions and forward-looking statements\n\nAll checks run in parallel for efficiency, with results aggregated into a comprehensive report.\n\n## Capabilities\n\n### Factual Verification\n- Claims verification against reliable sources\n- Evidence quality assessment\n- Source credibility evaluation\n- Contradiction detection\n\n### Mathematical Analysis\n- Calculation verification\n- Formula and statistical claim checking\n- Unit consistency analysis\n- Quantitative reasoning assessment\n\n### Forecasting Evaluation\n- Prediction methodology assessment\n- Uncertainty quantification review\n- Bias detection in forecasts\n- Evidence-to-conclusion strength analysis\n\n## Integration Approach\n\nThe agent provides:\n- **Domain-specific analysis** for each verification type\n- **Cross-domain consistency** checking for conflicting findings\n- **Integrated confidence** assessment across all three domains\n- **Prioritized issues** ranking by epistemic significance\n\n## Quality Dimensions\n\n- **Factual accuracy**: Are claims supported by evidence?\n- **Mathematical rigor**: Are calculations and formulas correct?\n- **Predictive quality**: Are forecasts methodologically sound?\n- **Logical consistency**: Do the domains align coherently?\n- **Evidence integration**: How well are multiple evidence types combined?\n\n---\n*This documentation is programmatically generated from source code. Do not edit manually.*\n"
} as const;

export type PluginId = keyof typeof pluginReadmes;

export function getPluginReadme(pluginId: string): string {
  const typedPluginId = pluginId as PluginId;
  return pluginReadmes[typedPluginId] || `# ${pluginId}\n\n*README content not available*`;
}
