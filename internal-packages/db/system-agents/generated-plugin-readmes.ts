/**
 * Auto-generated README content
 * Generated by scripts/generate-readmes.ts
 * DO NOT EDIT MANUALLY
 *
 * README Hash: 1869d2d8e480174d33ecdc1684d9e16168402a562663db7d4d19869556bea87c
 */

export const pluginReadmes = {
  "fact-checker": "# Fact Checker\n\nAn agent that identifies and verifies factual claims in documents, checking them against current knowledge and reliable sources. Provides detailed verdicts on claim accuracy with evidence-based reasoning.\n\n## Tools Used\n\n- **[Factual Claims Extractor](/tools/factual-claims-extractor)** - Extract and score verifiable factual claims from text\n- **[Fact Checker](/tools/fact-checker)** - Verify the accuracy of specific factual claims\n- **[Smart Text Searcher](/tools/smart-text-searcher)** - Find the location of text within documents using multiple search strategies including exact matching, fuzzy matching, quote normalization, partial matching, and LLM fallback for paraphrased or difficult-to-find text\n\n\n\n## Configuration\n\n**Processing Limits:**\n- Maximum facts to process: **30**\n- Maximum claims per chunk: **20**\n\n**Quality Thresholds:**\n- Minimum quality threshold: **25**\n\n**Importance Scoring:**\n- High importance: **60+**\n- Medium importance: **40+**\n\n**Checkability Scoring:**\n- High checkability: **50+**\n\n**Truth Probability Ranges:**\n- High: **90+**\n- Medium: **70-90**\n- Low: **50-70**\n- Very Low: **≤40**\n- Likely False: **≤30**\n\n## How It Works\n\nThe agent processes documents by:\n1. Extracting factual claims and statements\n2. Categorizing claims by type (statistical, historical, scientific, etc.)\n3. Verifying each claim against current knowledge\n4. Providing verdicts with supporting evidence\n5. Suggesting corrections for inaccurate claims\n\n## Verification Categories\n\n- **Factual claims**: General statements about events, people, or things\n- **Statistical data**: Numbers, percentages, measurements, trends\n- **Historical facts**: Dates, events, historical figures and contexts\n- **Scientific facts**: Research findings, natural phenomena, technical data\n- **Geographic facts**: Locations, distances, demographic information\n- **Current events**: Recent developments, ongoing situations\n\n## Verdict Types\n\n- **True**: Claim is accurate and well-supported\n- **False**: Claim is demonstrably incorrect\n- **Partially True**: Contains accurate elements but is misleading or incomplete\n- **Outdated**: Was true but no longer current\n- **Unverifiable**: Cannot be verified with available information\n- **Misleading**: Technically true but presented in a deceptive way\n\n---\n*This documentation is programmatically generated from source code. Do not edit manually.*\n",
  "math-checker": "# Math Checker\n\nAn agent that verifies mathematical statements, calculations, and formulas for accuracy. Combines computational verification with conceptual analysis to catch errors in arithmetic, algebra, statistics, and mathematical reasoning.\n\n## Tools Used\n\n- **[Math Expressions Extractor](/tools/math-expressions-extractor)** - Extract and analyze mathematical expressions from text, including error detection and complexity assessment\n- **[Math Validator (Hybrid)](/tools/math-validator-hybrid)** - Simple wrapper: try MathJS first, then LLM as fallback\n\n\n\n## How It Works\n\nThe agent analyzes mathematical content in documents by:\n1. Extracting mathematical expressions and statements\n2. Verifying calculations using both computational tools and mathematical reasoning\n3. Checking unit consistency and dimensional analysis\n4. Validating statistical claims and data interpretations\n5. Identifying conceptual errors in mathematical logic\n\n## When to Use\n\nThis plugin is called when documents contain:\n- Equations and formulas (2+2=4, E=mc², etc.)\n- Statistical calculations or percentages\n- Back-of-the-envelope calculations\n- Mathematical reasoning or proofs\n- Numerical comparisons (X is 3x larger than Y)\n- Unit conversions\n- Any discussion involving mathematical relationships\n\n## Routing Examples\n\n**Should Process:**\n> \"The population grew by 15% over the last decade, from 1.2M to 1.38M\"\n\n*Reason: Contains percentage calculation that should be verified*\n\n**Should Process:**\n> \"If we assume a 7% annual return, $10,000 invested today would be worth $19,672 in 10 years\"\n\n*Reason: Contains compound interest calculation*\n\n**Should NOT Process:**\n> \"Mathematics has been called the language of the universe\"\n\n*Reason: Discusses math conceptually but contains no actual math*\n\n## Capabilities\n\n- **Arithmetic verification** - Basic calculations, percentages, ratios\n- **Algebraic checking** - Equation solving, simplification, factoring\n- **Statistical validation** - Means, medians, correlations, significance tests\n- **Unit analysis** - Dimensional consistency, conversion errors\n- **Formula verification** - Scientific formulas, financial calculations\n- **Conceptual review** - Mathematical logic, proof steps, reasoning errors\n\n## Error Categories\n\n- **Calculation errors**: Incorrect arithmetic or computational mistakes\n- **Unit errors**: Dimensional inconsistencies or conversion mistakes\n- **Logic errors**: Flawed mathematical reasoning or invalid steps\n- **Notation errors**: Incorrect mathematical symbols or expressions\n- **Conceptual errors**: Misunderstanding of mathematical principles\n\n---\n*This documentation is programmatically generated from source code. Do not edit manually.*\n",
  "spelling-grammar": "# Spelling & Grammar Checker\n\nA sophisticated proofreading agent that combines language convention detection with Claude-based error analysis. Features adjustable strictness levels and automatic US/UK English convention handling.\n\n## Tools Used\n\n- **[Spelling & Grammar Checker](/tools/spelling-grammar-checker)** - Analyze text for spelling and grammar errors using Claude with advanced error detection\n\n\n\n## Configuration\n\n**Error Processing:**\n- Maximum errors per chunk: **20**\n- Minimum confidence threshold: **30**\n- High importance threshold: **50**\n\n**Language Convention Detection:**\n- Sample size for detection: **2000 characters**\n- Low confidence threshold: **0.8**\n- Low consistency threshold: **0.8**\n\n## How It Works\n\nFirst detects the document's language convention (US/UK/mixed) using convention detection, then analyzes text with Claude for error detection. The agent uses importance scoring (0-100) and confidence levels to prioritize errors, with configurable strictness levels (minimal/standard/thorough) that adjust the error detection threshold.\n\n## Capabilities\n\n- **Intelligent convention handling** - Can enforce specific US/UK spelling or adapt to mixed conventions\n- **Three strictness levels** for different use cases (minimal, standard, thorough)\n- **Returns exact error text** with concise corrections, importance scores, and confidence ratings\n- **Provides explanations** only for complex errors to reduce noise\n- **Line number tracking** for approximate error locations\n\n## Technical Details\n\n- **Strictness levels:** minimal (importance ≥51), standard (≥26), thorough (≥0)\n- **Convention modes:** US, UK, or auto-detect with mixed convention support\n- **Error scoring:** importance (0-100), confidence (0-100), with contextual descriptions\n- **Maximum errors:** 50 by default (configurable)\n\n---\n*This documentation is programmatically generated from source code. Do not edit manually.*\n",
  "forecast-checker": "# Forecast Checker\n\nAn agent that evaluates predictions, forecasts, and future-oriented claims for methodological soundness, evidence quality, and logical consistency. Assesses both quantitative and qualitative forecasting approaches.\n\n## Tools Used\n\n- **[Binary Forecasting Claims Extractor](/tools/binary-forecasting-claims-extractor)** - Extracts predictions and converts them to binary (YES/NO) questions. Scores on four dimensions: precision (how binary/specific), verifiability (can we check with public data), importance (centrality to argument), and robustness (how well-supported)\n- **[Binary Forecaster](/tools/binary-forecaster)** - Generate probability forecasts using multiple independent Claude analyses\n\n\n\n## Configuration\n\n**Forecast Generation:**\n- Number of forecasts per claim: **2** (for consensus)\n- Uses Perplexity for research: **false** (disabled by default)\n\n**Quality Scoring Dimensions (0-100):**\n- **Precision Score**: How specific and well-defined is the prediction?\n- **Verifiability Score**: How easily can the prediction be verified?\n- **Importance Score**: Significance and impact of the prediction\n- **Robustness Score**: How well-supported is the prediction?\n\n**Forecast Decision Threshold:**\n- Average quality score must be **high** across all dimensions for detailed analysis\n- Lower robustness increases likelihood of generating our own forecast (to check author's work)\n\n## How It Works\n\nThe agent analyzes forecasting content by:\n1. Identifying predictions and forward-looking statements\n2. Evaluating the evidence and reasoning supporting forecasts\n3. Assessing methodological approaches and assumptions\n4. Checking for logical consistency and bias\n5. Reviewing uncertainty quantification and confidence intervals\n6. Optionally generating independent forecasts to compare with author's predictions\n\n## Capabilities\n\n- **Prediction extraction** - Identifies explicit and implicit forecasts\n- **Method evaluation** - Assesses forecasting models and approaches\n- **Evidence analysis** - Reviews supporting data and assumptions\n- **Uncertainty assessment** - Evaluates confidence levels and ranges\n- **Bias detection** - Identifies overconfidence and systematic errors\n- **Logical consistency** - Checks internal coherence of predictions\n- **Comparative forecasting** - Generates independent predictions for validation\n\n## Analysis Categories\n\n- **Base rate neglect**: Ignoring historical frequencies\n- **Anchoring bias**: Over-reliance on initial estimates\n- **Overconfidence**: Unrealistic precision or certainty\n- **Poor calibration**: Misaligned confidence and accuracy\n- **Insufficient evidence**: Predictions without adequate support\n- **Methodology flaws**: Inappropriate forecasting techniques\n\n---\n*This documentation is programmatically generated from source code. Do not edit manually.*\n",
  "link-checker": "# Link Checker\n\nAn automated link validation agent that checks all external URLs in documents for accessibility, validity, and potential issues. Provides detailed reporting on broken links, redirects, and connection problems.\n\n## Tools Used\n\n- **[Link Validator](/tools/link-validator)** - Extracts and validates all URLs from a text, checking their accessibility and returning detailed validation results\n\n\n\n## Configuration\n\n**Processing Limits:**\n- Maximum URLs to check per document: **50**\n- Timeout per URL check: **10 seconds**\n- Maximum redirects to follow: **5**\n\n**Grading:**\n- Grade = (Working Links / Total Links) × 100\n- 100% = All links working\n- 0% = All links broken\n\n**Automatic Processing:**\n- This plugin runs on **all documents** automatically (no routing needed)\n- No LLM usage - pure HTTP validation (zero cost)\n\n## How It Works\n\nExtracts all external URLs from the document and validates each one by checking HTTP status codes and following redirects. The agent identifies broken links (404s), server errors (5xx), excessive redirects, and connection timeouts. Results are provided with exact text locations for easy correction.\n\n## Capabilities\n\n- **Comprehensive URL extraction** - Finds all external links in markdown, HTML, and plain text formats\n- **Status code validation** - Checks HTTP/HTTPS responses and categorizes issues\n- **Redirect chain analysis** - Follows and reports redirect chains up to reasonable limits\n- **Timeout handling** - Gracefully handles slow or unresponsive servers\n- **Bulk processing** - Efficiently validates multiple URLs with parallel checking\n- **Detailed reporting** - Provides exact link text and surrounding context for each issue\n\n## Error Categories\n\n- **404 Not Found**: Page doesn't exist\n- **5xx Server Errors**: Server-side problems\n- **Connection Timeouts**: Server too slow or unreachable\n- **Too Many Redirects**: Excessive redirect chains\n- **Network Errors**: DNS, SSL, or connection issues\n\n## Technical Details\n\n- **Supported protocols:** HTTP, HTTPS\n- **Validation method:** HTTP HEAD requests with GET fallback\n- **User agent rotation:** Attempts multiple user agents to avoid bot detection\n- **Cost:** $0 (no LLM usage)\n\n---\n*This documentation is programmatically generated from source code. Do not edit manually.*\n",
  "comprehensive-checker": "# Comprehensive Checker\n\nA thorough content validation agent that combines all verification tools except Spelling & Grammar checking. Runs Link Checker, Fact Checker, Math Checker, and Forecast Checker in parallel to provide complete content verification in a single pass.\n\n## How It Works\n\nThis agent performs multi-domain verification by running four specialized checkers simultaneously:\n1. **Link Checker**: Validates all external URLs for accessibility and validity\n2. **Fact Checker**: Verifies factual claims against current knowledge\n3. **Math Checker**: Checks calculations, formulas, and quantitative reasoning\n4. **Forecast Checker**: Evaluates predictions and forward-looking statements\n\nAll checks run in parallel for efficiency, with results aggregated into a comprehensive report.\n\n## Capabilities\n\n### Factual Verification\n- Claims verification against reliable sources\n- Evidence quality assessment\n- Source credibility evaluation\n- Contradiction detection\n\n### Mathematical Analysis\n- Calculation verification\n- Formula and statistical claim checking\n- Unit consistency analysis\n- Quantitative reasoning assessment\n\n### Forecasting Evaluation\n- Prediction methodology assessment\n- Uncertainty quantification review\n- Bias detection in forecasts\n- Evidence-to-conclusion strength analysis\n\n## Integration Approach\n\nThe agent provides:\n- **Domain-specific analysis** for each verification type\n- **Cross-domain consistency** checking for conflicting findings\n- **Integrated confidence** assessment across all three domains\n- **Prioritized issues** ranking by epistemic significance\n\n## Quality Dimensions\n\n- **Factual accuracy**: Are claims supported by evidence?\n- **Mathematical rigor**: Are calculations and formulas correct?\n- **Predictive quality**: Are forecasts methodologically sound?\n- **Logical consistency**: Do the domains align coherently?\n- **Evidence integration**: How well are multiple evidence types combined?\n\n---\n*This documentation is programmatically generated from source code. Do not edit manually.*\n"
} as const;

export type PluginId = keyof typeof pluginReadmes;

export function getPluginReadme(id: string): string {
  const typedId = id as PluginId;
  return pluginReadmes[typedId] || `# ${id}\n\n*README content not available*`;
}
